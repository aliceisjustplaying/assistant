networks:
  assistant-net:
    driver: bridge

services:
  # anthropic-proxy: OAuth proxy for Anthropic API
  anthropic-proxy:
    build:
      context: .
      dockerfile: Dockerfile.anthropic-proxy
    ports:
      - "4001:4001"
    environment:
      - PORT=4001
      - SESSION_SECRET=${ANTHROPIC_PROXY_SESSION_SECRET}
      - CLIENT_ID=9d1c250a-e61b-44d9-88ed-5944d1962f5e
      - REDIRECT_URI=https://console.anthropic.com/oauth/code/callback
      - OAUTH_BASE_URL=https://claude.ai
      - API_BASE_URL=https://api.anthropic.com/v1
    networks:
      - assistant-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4001/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Auth adapter: adds Accept-Encoding: identity to prevent gzip responses
  auth-adapter:
    image: oven/bun:latest
    working_dir: /app
    command: ["bun", "run", "src/auth-adapter.ts"]
    ports:
      - "4002:4002"
    environment:
      - ANTHROPIC_PROXY_INTERNAL_URL=http://anthropic-proxy:4001
      - AUTH_ADAPTER_PORT=4002
    volumes:
      - ./src/auth-adapter.ts:/app/src/auth-adapter.ts:ro
    networks:
      - assistant-net
    restart: unless-stopped
    depends_on:
      anthropic-proxy:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "bun", "-e", "fetch('http://localhost:4002/health').then(r => process.exit(r.ok ? 0 : 1))"]
      interval: 30s
      timeout: 10s
      retries: 3

  # LiteLLM: OpenAI-compatible API that proxies to auth-adapter -> anthropic-proxy
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    ports:
      - "4000:4000"
    environment:
      - ANTHROPIC_PROXY_SESSION_ID=${ANTHROPIC_PROXY_SESSION_ID}
    volumes:
      - ./litellm-config.yaml:/app/config.yaml:ro
    command: ["--config", "/app/config.yaml", "--port", "4000"]
    networks:
      - assistant-net
    restart: unless-stopped
    depends_on:
      auth-adapter:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:4000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3

  letta:
    image: letta/letta:latest
    ports:
      - "8283:8283"
    environment:
      # Use LiteLLM as OpenAI-compatible endpoint for Claude models
      - OPENAI_API_BASE=http://litellm:4000
      # Session ID is passed as API key to LiteLLM -> auth-adapter -> anthropic-proxy
      - OPENAI_API_KEY=${ANTHROPIC_PROXY_SESSION_ID}
    volumes:
      - letta-data:/var/lib/postgresql/data
    networks:
      - assistant-net
    restart: unless-stopped
    depends_on:
      litellm:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8283/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  app:
    build: .
    ports:
      - "3000:3000"
    environment:
      - PORT=3000
      - LETTA_BASE_URL=http://letta:8283
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - TELEGRAM_WEBHOOK_URL=${TELEGRAM_WEBHOOK_URL}
      - TELEGRAM_WEBHOOK_SECRET_TOKEN=${TELEGRAM_WEBHOOK_SECRET_TOKEN}
      - ANTHROPIC_PROXY_URL=http://anthropic-proxy:4001/v1
      - ANTHROPIC_PROXY_SESSION_ID=${ANTHROPIC_PROXY_SESSION_ID}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      # Tool webhook URL - Letta calls back to app via Docker network
      - TOOL_WEBHOOK_URL=http://app:3000
    volumes:
      - ./data:/app/data
    networks:
      - assistant-net
    restart: unless-stopped
    depends_on:
      letta:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  letta-data:
